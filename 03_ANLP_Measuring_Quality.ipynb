{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcon21/anlp-labs/blob/main/03_ANLP_Measuring_Quality.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7tNwXXJ_BBb"
      },
      "source": [
        "## Start by copying this into your Google Drive!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIBbWrF5_DXC"
      },
      "source": [
        "![Maastricht_University_logo.svg](data:image/svg+xml;base64,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)\n",
        "\n",
        "# Advanced Natural Language Processing Course - Tutorial Measuring Quality\n",
        "Author: Gijs Wijngaard\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14xe-BeO_G34"
      },
      "source": [
        "Version 2024-2025.1\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Welcome to the tutorial on measuring quality.\n",
        "\n",
        "\n",
        "The first step is to **enable GPU**. A GPU is a Graphical Processing Unit, capable of calculating vectors and matrices much faster than CPU units, like the one in your laptop. Since neural networks are basically made out of matrices, we gain serious speed improvements by using GPU's.\n",
        "\n",
        "We enable the GPU by clicking on *Runtime* in the menu above, then click *Change runtime type* and on the dropdown menu under *Hardware accelerator* we click *GPU*. Then click *Save*. If everything is correct, the below code should return *True*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1Ih_CLWBDAe"
      },
      "source": [
        "## Measuring Output Quality of a Classification Model\n",
        "We start with training a machine learning model first. We train a Transformers model on the most popular benchmark in natural language processing, named GLUE. This benchmark and its successor SuperGLUE are used in NLP research a lot to compare models to each other. Its a way for any model to test it if is performing well or not. You can find the benchmark [here](https://gluebenchmark.com/) and its [successor](https://super.gluebenchmark.com/) here. We are going to use one of its datasets as a task for our model to train on. Lets install some packages first:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKXKprMqw3vr"
      },
      "outputs": [],
      "source": [
        "!pip install -qq transformers datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8kTX3pbyUz7"
      },
      "source": [
        "GLUE consists of 11 datasets. Today, we will focus on only one of these, the Corpus of Linguistic Acceptability. This dataset is a dataset to test whether a model can recognize whether a sentence is actual English, or contains some spelling or grammatical mistakes. Lets import it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5b0RB-7x3wi"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "data = load_dataset(\"glue\", \"cola\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRk2A1pF8kv-"
      },
      "source": [
        "Lets display how our data looks like. This is a example of a correct sentence in our dataset (label = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWDJLBqN3MET"
      },
      "outputs": [],
      "source": [
        "data[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOMcPxe_rco4"
      },
      "source": [
        "Hereunder is an example of an incorrect sentence in our dataset (label = 0). You can't drink a pub right? That is for the model to recognize, can it find sentences that are incorrect."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HtZrBmg1iiP2"
      },
      "outputs": [],
      "source": [
        "data[\"train\"][18]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjHKtHLv8nnW"
      },
      "source": [
        "Lets now import our model. This is the first time we work with transformers models. Transformers is a library by HuggingFace. When working with transformer-based models, its one of the most convenient tools you can have. It supports all types of different trasnformer models, and you can download pretrained models from the transformers library to apply it to your own data. Normally, transformer models work well when trained on large datasets. With pretrained models, these models are already trained on large datasets thus do not need to be trained again. Its handy for applying state-of-the-art models on any problem you have.\n",
        "\n",
        "Let's use the most standard transformer model, that of BERT. We can use BERT for a variety of tasks, this time we will use it for sequence classification.  In a later lecture you will learn more about BERT and its applications. For our task, we want to know whether our data (which is a sequence) is a right english sentence or not (binary classification task).\n",
        "We also use `.to(device)` method to speed things up. If you don't want to wait long when predicting outputs using transformers, make sure you are on GPU in colab.\n",
        "\n",
        "You can ignore the warnings starting with `Some weights ...`, their just for letting you know what you can do with the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9EfQUBOA0Iws"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\").to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZHknOBjBVF_"
      },
      "source": [
        "Lets train our model first. We first define a dataloader. This function is used so that we can have large batches (`batch_size=16` in this case) and process multiple data inputs at once. This even speeds up the computation more. We first nullify our gradients. We put the data through the tokenizer, so that we get numbers instead of texts. Also an `attention_mask` is received from our tokenizer, so that our transformer model knows which part of the data to focus on: we pad the data to let it fit through the model. As model input, we feed the output of the tokenizer and a label, so that the model can compute a loss that defines how close we are to the label. We then backpropagate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-RFAXgKBkdk"
      },
      "outputs": [],
      "source": [
        "losses = []\n",
        "train_dataloader = torch.utils.data.DataLoader(data[\"train\"], batch_size=16)\n",
        "for item in tqdm(train_dataloader):\n",
        "    model.zero_grad()\n",
        "    inputs = tokenizer(item[\"sentence\"], padding=True, return_tensors=\"pt\").to(device)\n",
        "    result = model(**inputs, labels=item[\"label\"].to(device))\n",
        "    loss = result.loss\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    losses.append(loss.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evkmqJSL9f8o"
      },
      "source": [
        "Lets now define a simple prediction function. Remember, always train on a training set, and test your model on a validation set.\n",
        "\n",
        "We don't want to compute gradients, since we will not backpropagate the data (`torch.no_grad()`). We get the logits and move that onto the cpu with `.cpu().numpy()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECiVFJklPobo"
      },
      "outputs": [],
      "source": [
        "val_dataloader = torch.utils.data.DataLoader(data[\"validation\"], batch_size=16)\n",
        "results = []\n",
        "for item in tqdm(val_dataloader):\n",
        "    inputs = tokenizer(item[\"sentence\"], padding=True, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "    results.append(logits.argmax(dim=-1).cpu().numpy())\n",
        "results = [result for result_array in results for result in result_array]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsjhaQlG-XRy"
      },
      "source": [
        "These are our true values: The actual correct values of the validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0fD9Ckli4ru"
      },
      "outputs": [],
      "source": [
        "val_labels = [data[\"label\"] for data in data[\"validation\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIoMiLKv-dtf"
      },
      "source": [
        "We again can use the `accuracy_score` function from `scikit-learn`. This function predicts for us the `accuracy`: how many true positives and true negatives devided by all predictions we have."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxNeT9j47Lp1"
      },
      "outputs": [],
      "source": [
        "metrics.accuracy_score(val_labels, results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ANaR3VC-s_M"
      },
      "source": [
        "We also can plot a confusion matrix with `scikit-learn`. These values correspond to each of the 4 sectors. True positives,  true negatives, false positives, false negatives."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKkuKFpR78mi"
      },
      "outputs": [],
      "source": [
        "metrics.ConfusionMatrixDisplay.from_predictions(val_labels, results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErWnS4lHJZ0r"
      },
      "source": [
        "We can get the individual values using sklearn's `confusion_matrix`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ei6KRqSUJMCn"
      },
      "outputs": [],
      "source": [
        "tn, fp, fn, tp = metrics.confusion_matrix(val_labels, results).ravel()\n",
        "tn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGy2fo0d571Z"
      },
      "outputs": [],
      "source": [
        "fn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rlss923V-2i4"
      },
      "source": [
        "### Exercise 1.1\n",
        "> 1. Compute the following metrics by hand: $$Precision = \\frac{TP}{TP+FP}\\quad Recall = \\frac{TP}{(TP+FN)} \\quad Accuracy = \\frac{TP + TN}{TP + TN + FP + FN} \\quad F1 = 2 * \\frac{Precision * Recall}{Precision + Recall}$$ using the variables `tn`, `fp`, `fn`, `tp` above.\n",
        "2. Why is `accuracy` not a good metric for this dataset?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5inf6Wmp351"
      },
      "outputs": [],
      "source": [
        "# COMPUTE PRECISION, RECALL, ACCURACY AND F1 HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaTZ8kF2I2yE"
      },
      "source": [
        "ANSWER HERE:\n",
        "\n",
        "1.1.2:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fdx1EjSw7lj"
      },
      "source": [
        "### Exercise 1.2\n",
        "> 1. When do we prefer precision?\n",
        "2. When do we prefer recall?\n",
        "3. Give an examples of datasets that you could encounter where you would prefer one over the other."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LLjBnQXxUIL"
      },
      "source": [
        "ANSWER HERE:\n",
        "\n",
        "1.2.1:\n",
        "\n",
        "1.2.2:\n",
        "\n",
        "1.2.3:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8Ydqw1po5uR"
      },
      "source": [
        "### Exercise 1.3\n",
        "> 1. Now also compute $F0.5$ and $F2$ metrics.\n",
        "2. How do these compare to the $F1$ metric?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-B5vk9XYpEI9"
      },
      "source": [
        "ANSWER HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLPxG651pJ2-"
      },
      "outputs": [],
      "source": [
        "# COMPUTE F0.5 and F2 metrics here"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.3.2:"
      ],
      "metadata": {
        "id": "0SOYSJoOu3Xs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yPv7VwGMHjR"
      },
      "source": [
        "### Exercise 1.4\n",
        "> Instead of using `accuracy`, the GLUE benchmark uses a different metric for this dataset, Matthews correlation coefficient (also known as the Phi coefficient). $$MCC = \\frac{TP \\times TN - FP\\times FN}{\\sqrt{(TP+FP)\\times(TP+FN)\\times(TN+FP)\\times(TN+FN)}}$$ When computing this coefficient, we should get a value between 1 and -1, where 1 is a perfect prediction, 0 a random prediction and -1 a inverse prediction. Now compute also the MCC:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "saL4um3ANkD_"
      },
      "outputs": [],
      "source": [
        "# COMPUTE MCC HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fikKc8f_PcKm"
      },
      "source": [
        "\n",
        "## Using Model's probabilities\n",
        "We now computed some metrics using the predictions of the model: whether the model thinks its correct or incorrect. To calculate this, we took the `argmax()` of the logits. Let's focus one more time on what we did."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUbX6qRhHsGf"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer(next(iter(val_dataloader))[\"sentence\"], padding=True, return_tensors=\"pt\").to(device)\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits\n",
        "logits = logits.cpu().detach()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnGvb2beHsGg"
      },
      "source": [
        "We used these logits and for every pair of two we took whatever value is the highest (`argmax()`). Now we can also take the softmax, this then converts our values to make them sum together to 1 per prediction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7AeS2nKHsGg"
      },
      "outputs": [],
      "source": [
        "logits.softmax(dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDo7oeNvHsGg"
      },
      "source": [
        "As you can see, we now have a percentage per prediction of how sure the model is about that prediction, the the first value being the sentence is incorrect, the second value the sentence being correct. Lets now only take the last value of each prediction: we only need the chance that a value belongs to 1, if its higher than 50% probability it belongs so, if lower, it belongs to 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yp6M5wOcHsGg"
      },
      "outputs": [],
      "source": [
        "percentage = logits.softmax(dim=-1)[:, -1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIoTQ2zDHsGg"
      },
      "outputs": [],
      "source": [
        "percentage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBmSxN6aHsGg"
      },
      "source": [
        "Lets collect the percentages for the whole validation dataset, and plot the precision-recall curve:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4RhZ-j4HsGh"
      },
      "outputs": [],
      "source": [
        "percentages = []\n",
        "for item in tqdm(val_dataloader):\n",
        "    inputs = tokenizer(item[\"sentence\"], padding=True, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "    percentages.append(logits.softmax(dim=-1)[:, -1].cpu().numpy())\n",
        "percentages = np.array([result for result_array in percentages for result in result_array])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kASP4xfgHsGh"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import PrecisionRecallDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "PrecisionRecallDisplay.from_predictions(val_labels, percentages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEbXUFpFqfjD"
      },
      "source": [
        "Lets go over how this graph is calculated. On each step in the graph, we change the probability boundary, to get a different precision and recall value (also can be applied to TPR and FPR in ROC/AUC graphs) For example, normally we would just take 50% as boundary. All values that are above 0.5 are counted as being 1, all values below 0.5 are counted to 0. So these indices are predicted as false:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVDujUpmqeYE"
      },
      "outputs": [],
      "source": [
        "np.where(percentages < 0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIpVLU5LGkH_"
      },
      "outputs": [],
      "source": [
        "indices = np.where(percentages < 0.5, 0, 1)\n",
        "print(metrics.precision_score(val_labels, indices))\n",
        "print(metrics.recall_score(val_labels, indices))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXENkT3XITcw"
      },
      "source": [
        "Notice that this decision boundary is set to 0.5 with `np.where()`, we can change that to 0.9 or 0.4 or anything else."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbgjG9_nDq1m"
      },
      "outputs": [],
      "source": [
        "indices = np.where(percentages < 0.8, 0, 1)\n",
        "print(metrics.precision_score(val_labels, indices))\n",
        "print(metrics.recall_score(val_labels, indices))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSr_su-qHK5B"
      },
      "source": [
        "With the default 0.5 decision boundary, we get a somewhat high `precision`, but our `recall` is still low. Sometimes, we want one over the other, thus it makes sense to change the decision boundary. Say when you want to model to be absolutely sure about its predictions, you could then state that you only allow prediction values higher than 0.9 to be accepted (as 1) and the rest is 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKEp_BbGBKsp"
      },
      "source": [
        "Lets compute the values for this precision recall curve, we can also get the thresholds for this curve, so we know at which values the precision and recall was calculated (remember the threshold is our boundary). The amount of threshold values is equal to the number of unique percentages we have calculated from our dataset, e.g. `len(np.unique(percentages))`. By default we calculate the precision and recall for these thresholds, when calculating them for the graph above:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHxhf1iUEoLR"
      },
      "outputs": [],
      "source": [
        "precision, recall, thresholds = metrics.precision_recall_curve(val_labels, percentages)\n",
        "thresholds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENWs9tkbHsGi"
      },
      "source": [
        "### Exercise 2: 11-points Precision Recall\n",
        "Let's make an averaged 11-point precision recall graph (similar to the precision recall graph above, but now with only 11 points) , as explained in the lecture, and plot this below. For a refresher, [check here](https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-ranked-retrieval-results-1.html). You can use the [precision](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html) and [recall](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html) functions from scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyRrhaAh7I5_"
      },
      "outputs": [],
      "source": [
        "# ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdBKm6FGnhUG"
      },
      "source": [
        "## Metrics in Machine Translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbzFtbs7pwI6"
      },
      "source": [
        "We now continue to work with machine translation models. We again work with `transformers` pretrained models. This time we work on a translation dataset, that from news articles that are from 2 languages. We ask the model to translate from Dutch to English, and then we test how good the model performs. Don't worry if you don't know Dutch! You do not need to.\n",
        "\n",
        "Lets import the model first and split the data up in training and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6jfQ08_pvbV"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"opus_books\", \"en-nl\")\n",
        "dataset = dataset[\"train\"].train_test_split(test_size=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMCMz_7yrqGs"
      },
      "source": [
        "Lets check our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ei3VWVfLqfeY"
      },
      "outputs": [],
      "source": [
        "dataset[\"train\"][0][\"translation\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WD9y3Pq1CcUU"
      },
      "source": [
        "Alright, lets import an encoder-decoder model. In a later Colab in Course 8 of Advanced Natural Language Processing you will also work on Machine Translation, now we just want you to focus on computing the metrics for these notebooks, namely BLEU and METEOR.\n",
        "\n",
        "Down here we define a encoder-decoder model `t5-small`. We preprocess the input of these models so that it has a prompt, we can steer the model to come up with a solution then. We tokenize the input, truncate texts that are longer than 256 tokens, and batch them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4criQIPqkbl"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\").to(device)\n",
        "prefix = \"translate Dutch to English: \"\n",
        "def preprocess_function(examples):\n",
        "    inputs = [prefix + example[\"nl\"] for example in examples[\"translation\"]]\n",
        "    targets = [example[\"en\"] for example in examples[\"translation\"]]\n",
        "    return tokenizer(inputs, text_target=targets, max_length=256, truncation=True)\n",
        "train_data = dataset[\"train\"].map(preprocess_function, batched=True, remove_columns=[\"id\", \"translation\"])\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFM6h9HnE700"
      },
      "source": [
        "We put the data in a dataloader, which allows us for faster processing, we define an optimizer to train our model. The following code, for 1 epoch, takes about 10 minutes to train. Please make sure you are on a GPU (check instructions at the beginning of this notebook), else it will take longer!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TrywrNWuZOf"
      },
      "outputs": [],
      "source": [
        "train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=32, collate_fn=data_collator)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "for epoch in range(1):\n",
        "    for item in tqdm(train_dataloader):\n",
        "        model.zero_grad()\n",
        "        loss = model.forward(**item.to(device)).loss\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzj63aYcFPl2"
      },
      "source": [
        "We now take the test dataset and do exactly the same, this time we ask the model to come up with the english string he thinks is the translation of the dutch string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKvR0OKB2Av7"
      },
      "outputs": [],
      "source": [
        "def preprocess_test_function(examples):\n",
        "    return tokenizer([prefix + example[\"nl\"] for example in examples[\"translation\"]], max_length=256, truncation=True)\n",
        "test_data = dataset[\"test\"].map(preprocess_test_function, batched=True, remove_columns=[\"id\", \"translation\"])\n",
        "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=32, collate_fn=data_collator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1hBzo9DFeod"
      },
      "source": [
        "We let the model generate the english strings and then decode the output tokens back to strings with `batch_decode()`, we skip special tokens in this decoding process that are needed for generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "l4YRiYTDutYo"
      },
      "outputs": [],
      "source": [
        "outputs = []\n",
        "for item in tqdm(test_dataloader):\n",
        "    output = model.generate(**item.to(device))\n",
        "    outputs.append(output)\n",
        "translation_results = tokenizer.batch_decode([x for y in outputs for x in y], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeY5c5X_FxLB"
      },
      "source": [
        "Lets see what the results looks like!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8viMCmS89Zr8"
      },
      "outputs": [],
      "source": [
        "translation_results[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dR8O88hjGNLk"
      },
      "source": [
        "And now lets see what the original sentences look like (the ones the model should come close to when predicting):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0HgbZepA1ykO"
      },
      "outputs": [],
      "source": [
        "references = [data[\"translation\"][\"en\"] for data in dataset[\"test\"]]\n",
        "references[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hapn7-V3GVHW"
      },
      "source": [
        "Hmm, It looks like its far of, but some words are there at least. Now, how do we know for sure the model is performing correctly/incorrectly? We need a metric!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpltGbneGkUx"
      },
      "source": [
        "## Bilingual Evaluation Understudy (BLEU)\n",
        "BLEU is a good way to test how good these models perform. Normally you would just import a bleu metric from packages such as `NLTK` or `torchtext` and calcuate the score, but we are going to do it by hand (fun!). Lets implement $BLEU_1$, which means we focus ourselves on only unigrams (single words) (BLEU 1 is the BLEU score for n-grams where n = 1, so the UNIGRAMS. It look for single characters, not bi or trigrams). There is no need to include the more complex brevity penalties or other complex variation on the default BLEU-1.\n",
        "\n",
        "Our candidate sentences are defined in `results`. Our reference sentences are defined in `references`.\n",
        "\n",
        "We iterate over the results and references, and compute on each iteration for both result and reference the ngrams. We do this by counting the words/tokens in the sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qisw4SGdacml"
      },
      "source": [
        "\n",
        "### Exercise 3: BLEU\n",
        "> 1. Implement the BLEU score yourself, by using the formulas from [wikipedia](https://en.wikipedia.org/wiki/BLEU) or the slides.\n",
        "2. Apply the BLEU score on one of the references and candidates to see if your implementation works.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pA3zwnNZahVX"
      },
      "outputs": [],
      "source": [
        "# WRITE BLEU SCORE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9jZ27R9ard8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAbcbFdYarQg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaTkkCEJbHlb"
      },
      "source": [
        "## Language Model Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6DGF_YT8cnHK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lsx3eVZavvE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jkp_C_6UKxB3"
      },
      "source": [
        "We can also know how good our model performs by calculating the perplexity. For an encoder-decoder model, the model is trained on a cross entropy loss, so we just do:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KI6NlB4K4Ml"
      },
      "outputs": [],
      "source": [
        "perplexity = torch.exp(loss)\n",
        "perplexity.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVnQsmD6K6bQ"
      },
      "source": [
        "### Exercise 4: Perplexity\n",
        "> 1. Research what perplexity is, and if our score above is good or not.\n",
        "2. Are there also other metrics or ways how we can measure language models?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YMN03rzsUel"
      },
      "source": [
        "ANSWER HERE:\n",
        "\n",
        "4.1:\n",
        "\n",
        "\n",
        "4.2:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBamJ69fTENU"
      },
      "source": [
        "#Submission\n",
        "Please share your Colab notebook by clicking File on the top-left corner. Click under Download on Download .ipynb and upload that file to Canvas."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}