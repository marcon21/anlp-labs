{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcon21/anlp-labs/blob/main/01_ANLP_Tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start by copying this into your Google Drive!!"
      ],
      "metadata": {
        "id": "3VgDWmVan1SO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![maastricht-university-logo.jpeg](data:image/jpeg;base64,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)\n",
        "#Faculty of Science and Engineering - Department of Advanced Computer Sciences\n",
        "# Course Advanced Natural Language Processing (ANLP) - Tutorial Tokenization"
      ],
      "metadata": {
        "id": "8S4ipRELHrMV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By Jan Scholtes\n",
        "Version 2024-2025.1\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Welcome to the tutorial on Tokenization. In this notebook you will learn how to preprocess text into tokens.\n",
        "\n",
        "This is the basis of any Information Retrieval, Text Mining or NLP process. Tokenization is closely related to sentence detection, stemming, lemmatization and is part of the large NLP research topic named morphology.\n",
        "\n",
        "Tokenization is highly language dependent. In this tutorial we focus on Western-European languages.\n",
        "\n",
        "In this notebook, we will use the Stanford NLTK library.\n",
        "\n"
      ],
      "metadata": {
        "id": "0ZVnJSqfnp_v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text extraction and cleanup is an important component of real-world NLP systems. Text extraction allows one to extract text from various electronic file formats (TXT, HTML, XML, PDF, DOCX, XLSX, PPTX, ...) and deals with the encoding of the characters (Unicode, UTF-8, Code pages or ACSII).\n",
        "\n",
        "Libraries such as BeautifulSoup, Scapy or Selenium can assist you with webscraping and parsing text from HTML and XML.\n",
        "\n",
        "You can run the example hereunder to see how a Webpage is scraped and parsed into tags, which can subsequently be questioned (remove the # before this line:\"pprint(soupified.prettify())\" to see the entire HTML file (it is long)."
      ],
      "metadata": {
        "id": "hgRVOWBFrjcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# making the necessary imports,\n",
        "from pprint import pprint\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "myurl = \"https://stackoverflow.com/questions/415511/how-to-get-the-current-time-in-python\"\n",
        "html = requests.get(myurl, headers={'user-agent': 'Scraper'}).text # query the website\n",
        "soupified = BeautifulSoup(html, 'html.parser') # parse the html in the 'html' variable, and store it in Beautiful Soup format\"\n",
        "\n",
        "#pprint(soupified.prettify())      # for printing the full HTML structure of the webpage\n",
        "\n",
        "question = soupified.find(\"div\", {\"class\": \"question\"}) # find the nevessary tag and class which it belongs to\n",
        "questiontext = question.find(\"div\", {\"class\": \"s-prose js-post-body\"})\n",
        "print(\"Question: \\n\", questiontext.get_text().strip())\n",
        "answer = soupified.find(\"div\", {\"class\": \"answer\"}) # find the nevessary tag and class which it belongs to\n",
        "answertext = answer.find(\"div\", {\"class\": \"s-prose js-post-body\"})\n",
        "print(\"Best answer: \\n\", answertext.get_text().strip())"
      ],
      "metadata": {
        "id": "bEmUPqnWrVx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PDF can be quite challenging, especially from a formatting point of view. There are also many PDF reverse engineered formats that do not follow the official PDF guideliness completely. For popular formats from Microsoft, Google, Open Office and other vendors, there are several open source libraries to exract text and meta data. For more obscure file types, one has to fall back to commercial solutions such as Oracle Outside In, but these can be expensive.\n",
        "\n",
        "Encoding normalization is important to map various variants of code pages (https://en.wikipedia.org/wiki/Code_page ), ASCII and other encodings to one common Unicode format (https://home.unicode.org/). UTF-8 is the most used one.\n",
        "\n",
        "In this tutorial, we presume all this has been done and we can start with UTF-8 text files that only contain basic line (CR-LF) and tab formatting."
      ],
      "metadata": {
        "id": "pboMwIeQwxil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NLTK"
      ],
      "metadata": {
        "id": "fPfiHmSf7dmT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we load NLTK"
      ],
      "metadata": {
        "id": "Tl18k70t1-EM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt') # load tokenization"
      ],
      "metadata": {
        "id": "c44v65PO2AeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK also contains many text corpora. Let's import the movie reviews."
      ],
      "metadata": {
        "id": "fD3v4P3lOZyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('movie_reviews')\n",
        "from nltk.corpus import movie_reviews\n",
        "movie_reviews.readme()"
      ],
      "metadata": {
        "id": "Gk1RSb6VOnDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see what is in there"
      ],
      "metadata": {
        "id": "XVqQvmAZPDeL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw = movie_reviews.raw()\n",
        "print(raw[0:1000:1]) # print first 1000 chars\n"
      ],
      "metadata": {
        "id": "POr_DqyYPFKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see if we can detect the long tail that is typical for natural language. First we seperate the text in indivudual words, then we run a frequency analsyis on the results."
      ],
      "metadata": {
        "id": "EdCLI7XPPzLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = movie_reviews.words()\n",
        "print(corpus)\n",
        "freq_dist = nltk.FreqDist(corpus)\n",
        "print(freq_dist)\n",
        "print(freq_dist.most_common(50))\n",
        "freq_dist.plot(500)"
      ],
      "metadata": {
        "id": "q-iCAp-ZP3ZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 1:\n",
        "a. What kind of a relation do you observe? A lineair, polynomial or power relation?  \n",
        "\n",
        "b. What are the most frequent words?\n",
        "\n",
        "c. Do these words have a clear meaning or can they have multiple meaning depending on the context?\n",
        "\n",
        "d. Can these words also be used for (long) distance (complex) references?\n",
        "\n",
        "e. What does this mean for your NLP algorithm?"
      ],
      "metadata": {
        "id": "ID49Xu16QW8z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANSWER HERE:\n",
        "\n",
        "1.a\n",
        "\n",
        "1.b\n",
        "\n",
        "1c.\n",
        "\n",
        "1d.\n",
        "\n",
        "1e.\n",
        "\n"
      ],
      "metadata": {
        "id": "v4Tq_8i-Ut3B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can also observe, punctuation characters such as .  and , and other ones (: ; \" \" ? ! ) are still in there. This is where tokenization comes in. Tokenization removes punctuations where they are used as sentence and phrase seperation, but leaves them where they are part of a token (e.g. an email address or abbreviation).  "
      ],
      "metadata": {
        "id": "ApQiVxWzQgqs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sentence Detection"
      ],
      "metadata": {
        "id": "7AfVoIBc7gh6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentences are the basic components of human language. For all of the applications we discuss in this course (e.g. Machine Translation, Abstracting, ...) the algorithms used to process language presume that the data consists of correct linguistic sentences. When machine learning is applied, this is also done using correct linguistic sentences. Therefor, sentence detection is extremely important in NLP. If language does not have the form of linguistic sentences, then many of the algorithms we teach in this course will not work. This is the case when dealing with tables, graphs or certain types of headers and footers (e.g. an address in the top of a letter).\n",
        "\n",
        "Next, we load the NLTK tokenizer for sentences (sent_tokenize) and for words (word_tokenize)"
      ],
      "metadata": {
        "id": "L2oTaS-W2KuL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize"
      ],
      "metadata": {
        "id": "y9gnIhNd2JDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_text = \"The Department of Advanced Computing Sciences - sometimes abbreviated as DACS - \\n is Maastricht University’s largest and oldest department \\n broadly covering the fields of artificial intelligence, data science, computer science, \\n mathematics and robotics. We maintain a large network of public and \\n private partners through our research collaborations and through the \\n award-winning KE@Work programme. In addition, our staff teaches approximately 800 bachelor’s and master’s \\n students in 3 specialized study programmes in Data Science \\n and Artificial Intelligence. The Department of Advanced Computing Sciences \\n  is the new joint identity of the Institute of Data Science (IDS) and the former \\n Department of Data Science and Knowledge Engineering (DKE).\"\n",
        "print(my_text)\n"
      ],
      "metadata": {
        "id": "T6CLqSUA29BU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word Detection aka Tokenization"
      ],
      "metadata": {
        "id": "MA5suVwx7lHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_sentences = sent_tokenize(my_text)\n",
        "# print(my_sentences) # print entire list unformatted\n",
        "print(\"\\n\")\n",
        "for x in range(len(my_sentences)):\n",
        "    print(my_sentences[x]+\"\\n\")"
      ],
      "metadata": {
        "id": "P48pIjx8XW2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given a character sequence and a defined document unit, tokenization is the task of chopping it up into pieces, called tokens , perhaps at the same time throwing away certain characters, such as punctuation.\n",
        "A token is an instance of a sequence of characters in some particular document that are grouped together as a useful semantic unit for processing. A type is the class of all tokens containing the same character sequence."
      ],
      "metadata": {
        "id": "LqK3BNkLX5X_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in my_sentences:\n",
        "    print(\"Sentence: \"+str(sentence))\n",
        "    my_words = word_tokenize(sentence)\n",
        "    print(\"Tokens: \")\n",
        "    for x in range(len(my_words)):\n",
        "      print(\"    \"+str(my_words[x]))\n"
      ],
      "metadata": {
        "id": "ISHpq9JAY4XX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can observe, there are still punctuation in the list of tokens. In NLTK these can be removed by using a regular expression."
      ],
      "metadata": {
        "id": "tnvfN0OsairB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "new_tokenizer = RegexpTokenizer(r'\\w+')\n",
        "for sentence in my_sentences:\n",
        "    print(\"Sentence: \"+str(sentence))\n",
        "    my_words = new_tokenizer.tokenize(sentence)\n",
        "    print(\"Tokens: \")\n",
        "    for x in range(len(my_words)):\n",
        "      print(\"    \"+str(my_words[x]))"
      ],
      "metadata": {
        "id": "uERolN74aqXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 2:\n",
        "As you can observe, our tokenizer cannot deal with tokens such as \"Bachelor's\" or \"Master's\". This has to do with the fact that the NLTK tokenizer does not recognize 's as 'its'. It is not part of the tokenizer's code to deal with that. When dealing with chemical formulas, RNA/DNA, part numbers, phone numbers (with spaces, -, . etc) we see similar problems. When there are problems with the tokenization, the errors propagate in subsequent parts of the processing.\n",
        "\n",
        "We can address this problem by applying preprocessing and normalization of such tokens.\n",
        "\n",
        "Write some code to preprocess 's into its and then do proper tokenization."
      ],
      "metadata": {
        "id": "YhgIu4uKcbmv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "YOUR ANSWER HERE"
      ],
      "metadata": {
        "id": "oL39SsVJc3QQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate a Vocabulary"
      ],
      "metadata": {
        "id": "B-OCi_nxIPYP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A vocabulary is a data structure containing every unique word used in the corpus only once and in alphabetical order. This can be used as a dictionairy in NLP or as the basis of a search index in information retrieval."
      ],
      "metadata": {
        "id": "STaUFJPJJIrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_tokens = new_tokenizer.tokenize(my_text.lower()) #use the tokenizer that removes punctuation\n",
        "vocab = sorted(set(corpus_tokens))\n",
        "print(vocab)\n",
        "print(\"Tokens:\", len(corpus_tokens))\n",
        "print(\"Vocabulary:\", len(vocab))"
      ],
      "metadata": {
        "id": "guS6cWr6ISTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Stop Words"
      ],
      "metadata": {
        "id": "51E04B3g7pz6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the past, when computer resources were still limited, highly frequent words were often removed in information retrieval applications. These are named stop-words or noise-words. These are words such as \"the, on, in, a, be, or, and, an, for, to, ...\". If such a word is removed, one can no longer search for them. Imagine searching for \"to be or not to be\", which is no longer after noise word removal.\n",
        "\n",
        "In text-mining and advanced NLP (especially when using context-sensitive deep-leaning models), these words often contain important clues on the meaning of language. Removing them will completely destroy the performance of the models.\n",
        "\n",
        "So, these days as computer resources are much larger and context sensitive models need these words for disambiguation, noise words are no longer removed.\n",
        "\n",
        "But let's try how to remove them using NLTK."
      ],
      "metadata": {
        "id": "JxnKcmMt4tDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "print(\"Stopwords from NLTK:\", stopwords.words('english'))\n",
        "stop_words_nltk = set(stopwords.words('english'))\n",
        "# we use the token list without punctuations\n",
        "print(\"Tokenized corpus:\",corpus_tokens)\n",
        "#now remove stopwords\n",
        "tokenized_corpus_without_stopwords = [i for i in corpus_tokens if not i in stop_words_nltk]\n",
        "print(\"Tokenized corpus without stopwords:\",tokenized_corpus_without_stopwords)"
      ],
      "metadata": {
        "id": "95Xp0Zlc5qOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 3:\n",
        "\n",
        "ANSWER HERE\n",
        "\n",
        "3.a What do you observe with respect to case sensitivity of proper names and words such as \"I\".\n",
        "\n",
        "3.b How can we solve that?\n",
        "\n",
        "3.c Could this actions also lead to unwanted side effects?\n",
        "\n"
      ],
      "metadata": {
        "id": "_7bgF2uT7P0E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Stemming"
      ],
      "metadata": {
        "id": "jsjAz4lF7Y0K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming is the process of removing suffixes and reducing the word to some base form such that all different variations of a word can be represented by one form. Stemming uses rules and may not always result in the correct linguistic base form. However, it is fast and therefor often used by search engines. As we discussed in the lecture, a well-known stemmer for the English language is the Porter stemmer.\n",
        "\n",
        "Let's try it ..."
      ],
      "metadata": {
        "id": "6UYjccj-RvK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer= PorterStemmer()\n",
        "print(\"before stemming -> after stemming\")\n",
        "for word in corpus_tokens:\n",
        "  print(str(word) + \" -> \" + str(stemmer.stem(word)))\n"
      ],
      "metadata": {
        "id": "-o3TscOrSXGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, \"students\" is converted into \"student\", but \"Science\" is converted into \"scien\". There are other non-linguistically correct transformations."
      ],
      "metadata": {
        "id": "6XnibDb_Xs1y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lemmatization"
      ],
      "metadata": {
        "id": "EDc2SVqq7w2a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This why we prefer to use lemmatization for linguistic applications other than search engines. Lemmatization is the process of mapping all tokens to its base-linguistic form: the \"lemma\". So \"better\" should be converted to \"good\" and \"is\" to \"be\"."
      ],
      "metadata": {
        "id": "Kav2cXbpYKa9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')  # downloading wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "print(\"before lemmatization -> after lemmarization\")\n",
        "for word in corpus_tokens:\n",
        "  print(str(word) + \" -> \" + str(lemmatizer.lemmatize(word)))"
      ],
      "metadata": {
        "id": "zv5796gKY8C7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can observe, only plurals and other basic operations are performed. But \"is\" not converted to \"be\". Neither are several verb inflections. This is because Lemmatization requires more linguistic knowledge: it need to know whether we are dealing with, for instance, a verb, noun or a adjectice. We call these gramatical roles \"part-of-speech\" or POS tags. These will be discussed in the next lecture: Syntax and Semantics."
      ],
      "metadata": {
        "id": "t4l3z9ajbeIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(lemmatizer.lemmatize('better'))\n",
        "print(lemmatizer.lemmatize('better',pos='a')) # a for Adjective\n",
        "print(lemmatizer.lemmatize('is'))\n",
        "print(lemmatizer.lemmatize('is',pos='v'))  # v for Verb\n",
        "print(lemmatizer.lemmatize('is',pos='a'))\n",
        "print(lemmatizer.lemmatize('is',pos='n'))  # n for Noun\n",
        "print(lemmatizer.lemmatize('richer',pos='n'))\n",
        "print(lemmatizer.lemmatize('richer',pos='a'))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F9Cq2Xx2b4I6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text-Normalization"
      ],
      "metadata": {
        "id": "QE6ntyME72S0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In social media, one can run in short-cuts, slang, hash-tags, or emoticons. These can be concerted to their textual forms. Phone numbers, dates and monetary amounts can be written in many different forms. Sometimes, one can even decide to convert all text to either lower case or upper case. This may cause problems in some applications and should be used carefully. We will discuss this in more detail in the course Text Mining, where this is more important."
      ],
      "metadata": {
        "id": "eyicUNL2enhr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Language Detection"
      ],
      "metadata": {
        "id": "NeGSKRnx78Ry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Almost all NLP models and algorithms are very language specific: this means that one can only use them with the intenred language. Using them on other language will result in random behavior.  \n",
        "\n",
        "So, language detection (often per sentence or minimally per paragrpah) is essential for any type of NLP application to perform correctly!"
      ],
      "metadata": {
        "id": "IhONHqV2fiU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langdetect\n",
        "from langdetect import detect, detect_langs\n",
        "def language_detection(text, method = \"single\"):\n",
        "  if(method.lower() != \"single\"):\n",
        "    result = detect_langs(text)\n",
        "  else:\n",
        "    result = detect(text)\n",
        "  return result\n",
        "\n",
        "multilingual_text = \"Elle est vraiment éfficace dans la détection de langue.\"\n",
        "print(language_detection(multilingual_text))\n",
        "multilingual_text = \"Het is enorm makkelijk om een taal te herkennen!\"\n",
        "print(language_detection(multilingual_text))\n",
        "multilingual_text = \"Es ist wirklich effektiv bei der Spracherkennung.\"\n",
        "print(language_detection(multilingual_text))\n",
        "multilingual_text = \"Nó thực sự hiệu quả trong việc phát hiện ngôn ngữ.\"\n",
        "print(language_detection(multilingual_text))\n",
        "multilingual_text = \"إنه فعال حقًا في اكتشاف اللغة.\"\n",
        "print(language_detection(multilingual_text))"
      ],
      "metadata": {
        "id": "njzaEIDBgFAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transliteration"
      ],
      "metadata": {
        "id": "OmCN9E977_mi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transliteration refers to the method of mapping from one system of writing to another based on phonetic similarity. With this tool, you type in Latin letters (e.g. a, b, c etc.), which are converted to characters that have similar pronunciation in the target language. For transliteration, you need to select the target language. So, results for a transliteration of a Arabic name into English, French or German can be very different for similar names."
      ],
      "metadata": {
        "id": "munkN882hiya"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Лев Николаевич Толстой\n",
        "\n",
        "results in different forms of transliteration for different target languages:\n",
        "\n",
        "Lev Nikolayevich Tolstoy\n",
        "\n",
        "Léon Tolstoï\n",
        "\n",
        "Lev Tolstoj\n",
        "\n",
        "León Tolstó\n",
        "\n",
        "Lev Tolstoy\n",
        "\n",
        "Lav Tolstoj\n",
        "\n",
        "Lev Tolsto\n",
        "\n",
        "Liuni Tolstoi\n",
        "\n",
        "Ļevs Tolstojs\n",
        "\n",
        "Levs Tuolstuos\n",
        "\n",
        "...\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1ciweSsXiu0B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Python library for transliteration can be found here: https://pypi.org/project/transliterate/. We will discuss this in more detail in the lecture on Machine Translation."
      ],
      "metadata": {
        "id": "QvS5d3LEiNcp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 4: Final reflections on tokenization.\n",
        "\n",
        "As we have seen in this tutorial, tokenization is a very important component of NLP. Any error in this phase will propagate in the rest of the processing. Tokenization errors can occur from:\n",
        "- wrong character set (character encodings not in tokenizer)\n",
        "- wrong language\n",
        "- wrong sentence detection or no sentences at whole\n",
        "- un-expected tokens (chemical formula, RNA/DNA. part numbers, ...)\n",
        "- wrongly dealing with abbreviations\n",
        "- different spelling variations (212)-123.4567 vs 212 123 4567 for NYC phone numbers.\n",
        "- text mutilation by using removal of stop words or stemming\n",
        "- typos, spelling variations and errors\n",
        "\n",
        "4.a How would you identify potential tokenization problems in your NLP project?\n",
        "\n",
        "4.b In what order would you do the detection and pre-processing to deal with the above problems in your NLP project?"
      ],
      "metadata": {
        "id": "vV7f89o0VYzo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "YOUR ANSWER HERE\n",
        "\n",
        "4a.\n",
        "\n",
        "4b."
      ],
      "metadata": {
        "id": "5ol5sN4fVfdn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Submission\n",
        "Please share your Colab notebook by clicking File on the top-left corner. Click under Download on Download .ipynb and upload that file to Canvas."
      ],
      "metadata": {
        "id": "ZdS9JFQ5UXkX"
      }
    }
  ]
}